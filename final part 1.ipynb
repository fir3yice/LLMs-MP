{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6887c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"\"\"\n",
    "You are an expert translation evaluator for English ↔ Filipino translations. Assess the quality of the given translation from English to Filipino based only on the provided inputs.\n",
    "\n",
    "Evaluate according to these six criteria (1 point each):\n",
    "1. Accuracy - Correct meaning, intent, and details.\n",
    "2. Fluency - Grammatically correct, natural, and idiomatic Filipino.\n",
    "3. Coherence - Logical flow and structure matching the source.\n",
    "4. Cultural Appropriateness - Respects Filipino norms, idioms, and sensitivities.\n",
    "5. Guideline Adherence - Follows any domain-specific terminology and style.\n",
    "6. Completeness - Translates all elements without omission/addition.\n",
    "\n",
    "Scoring:\n",
    "- Total points: 0-6  \n",
    "- Normalize to 1-5 scale:  \n",
    "  • 5 = Excellent (5-6 points)  \n",
    "  • 3-4 = Good (3-4 points)  \n",
    "  • 1-2 = Poor (0-2 points)  \n",
    "\n",
    "Output format (follow exactly):\n",
    "Score: <number from 1-5>  \n",
    "Label: <\"excellent\", \"good\", or \"poor\">  \n",
    "Reasoning:  \n",
    "Accuracy: <your comment>  \n",
    "Fluency: <your comment>  \n",
    "Coherence: <your comment>  \n",
    "Cultural Appropriateness: <your comment>  \n",
    "Guideline Adherence: <your comment>  \n",
    "Completeness: <your comment>  \n",
    "\n",
    "Be concise but clear in reasoning. Base your evaluation only on the provided inputs.\n",
    "\n",
    "---\n",
    "\n",
    "### Example 1\n",
    "Source (English): The meeting will start at 9 a.m. sharp.  \n",
    "Translation (Filipino): Ang pulong ay magsisimula nang eksakto alas-nwebe ng umaga.  \n",
    "\n",
    "Score: 5  \n",
    "Label: excellent  \n",
    "Reasoning:  \n",
    "Accuracy: Fully conveys the meaning and time detail.  \n",
    "Fluency: Grammatically correct and natural phrasing.  \n",
    "Coherence: Matches structure and intent of the source.  \n",
    "Cultural Appropriateness: No issues; standard formal Filipino.  \n",
    "Guideline Adherence: Appropriate for formal context.  \n",
    "Completeness: All information is included without additions.  \n",
    "\n",
    "---\n",
    "\n",
    "### Example 2\n",
    "Source (English): She gave him a cold look before leaving.  \n",
    "Translation (Filipino): Tiningnan niya siya nang malamig bago umalis.  \n",
    "\n",
    "Score: 3  \n",
    "Label: good  \n",
    "Reasoning:  \n",
    "Accuracy: Literal translation captures meaning but slightly awkward.  \n",
    "Fluency: Understandable, but “nang malamig” sounds unnatural; “matamang tingin” or “tingin na walang emosyon” would be smoother.  \n",
    "Coherence: Sequence is clear and logical.  \n",
    "Cultural Appropriateness: Acceptable, though more idiomatic options exist.  \n",
    "Guideline Adherence: Fits general domain but not stylistically refined.  \n",
    "Completeness: All information is present.  \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Source (English): {english_text}\n",
    "Translation (Filipino): {filipino_text}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555fc3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 57 rows from data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "filename = \"data.csv\"\n",
    "data = pd.read_csv(filename, encoding='utf-8')\n",
    "\n",
    "data = data.drop(columns=['Contributor'], errors='ignore')\n",
    "print(f\"Loaded {len(data)} rows from {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "363764c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source Text (English)</th>\n",
       "      <th>Target Text (Filipino)</th>\n",
       "      <th>Score</th>\n",
       "      <th>Rater 1 Explanation</th>\n",
       "      <th>Rater 2 Explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The children laughed and played under the afte...</td>\n",
       "      <td>Ang mga bata ay nagtawanan at naglaro sa ilali...</td>\n",
       "      <td>4</td>\n",
       "      <td>Accurate, fluent, and natural translation. Cap...</td>\n",
       "      <td>Just slight error due to the literal translati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>She took a break to gather her thoughts.</td>\n",
       "      <td>Nagpahinga siya para mag-isip-isip.</td>\n",
       "      <td>4</td>\n",
       "      <td>The translation is accurate. It was able to ca...</td>\n",
       "      <td>The translation would have been better if the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The algorithm efficiently identifies patterns ...</td>\n",
       "      <td>Mabisang kinikilala ng algoritmo ang mga patte...</td>\n",
       "      <td>3</td>\n",
       "      <td>The translation of \"identifies\" as \"kinikilala...</td>\n",
       "      <td>The translation would have been better if the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data normalization helps improve model perform...</td>\n",
       "      <td>Tumutulong sa pagpabuti ng model ang normalisa...</td>\n",
       "      <td>5</td>\n",
       "      <td>The translated text is natural and captures th...</td>\n",
       "      <td>The translation didn't literally translated th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alam mo ma'am masaya naman topics natin sa phi...</td>\n",
       "      <td>You know, ma'am, we have a lot of fun philosop...</td>\n",
       "      <td>4</td>\n",
       "      <td>flawed translation is close, but failed to tra...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Source Text (English)  \\\n",
       "0  The children laughed and played under the afte...   \n",
       "1           She took a break to gather her thoughts.   \n",
       "2  The algorithm efficiently identifies patterns ...   \n",
       "3  Data normalization helps improve model perform...   \n",
       "4  alam mo ma'am masaya naman topics natin sa phi...   \n",
       "\n",
       "                              Target Text (Filipino)  Score  \\\n",
       "0  Ang mga bata ay nagtawanan at naglaro sa ilali...      4   \n",
       "1                Nagpahinga siya para mag-isip-isip.      4   \n",
       "2  Mabisang kinikilala ng algoritmo ang mga patte...      3   \n",
       "3  Tumutulong sa pagpabuti ng model ang normalisa...      5   \n",
       "4  You know, ma'am, we have a lot of fun philosop...      4   \n",
       "\n",
       "                                 Rater 1 Explanation  \\\n",
       "0  Accurate, fluent, and natural translation. Cap...   \n",
       "1  The translation is accurate. It was able to ca...   \n",
       "2  The translation of \"identifies\" as \"kinikilala...   \n",
       "3  The translated text is natural and captures th...   \n",
       "4  flawed translation is close, but failed to tra...   \n",
       "\n",
       "                                 Rater 2 Explanation  \n",
       "0  Just slight error due to the literal translati...  \n",
       "1  The translation would have been better if the ...  \n",
       "2  The translation would have been better if the ...  \n",
       "3  The translation didn't literally translated th...  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d130e5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Prompt:\n",
      "\n",
      "Source (English): She took a break to gather her thoughts.\n",
      "Translation (Filipino): Nagpahinga siya para mag-isip-isip.\n",
      "\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "def construct_prompt(row, prompt_template):\n",
    "    english_text = row['Source Text (English)']\n",
    "    filipino_text = row['Target Text (Filipino)']\n",
    "    score = row.get('Score', None)  \n",
    "\n",
    "    prompt = prompt_template.format(english_text=english_text, filipino_text=filipino_text)\n",
    "    \n",
    "    return prompt, filipino_text, score\n",
    "\n",
    "sample_prompt, translated_text, score = construct_prompt(data.iloc[1], prompt_template)\n",
    "\n",
    "print(\"Sample Prompt:\")\n",
    "print(sample_prompt) \n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35d019f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"\"\n",
    "\n",
    "def query_gemini(prompt):\n",
    "    client = genai.Client(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
    "    model = \"gemini-2.0-flash\"\n",
    "    \n",
    "    contents = [\n",
    "        types.Content(\n",
    "            role=\"user\",\n",
    "            parts=[types.Part.from_text(text=prompt)],\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    generate_content_config = types.GenerateContentConfig(\n",
    "        temperature=0.0,\n",
    "        system_instruction=system,\n",
    "        response_mime_type=\"text/plain\",\n",
    "    )\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "        model=model,\n",
    "        contents=contents,\n",
    "        config=generate_content_config,\n",
    "    )\n",
    "    \n",
    "    return response.text.strip() if response and response.text else \"unknown\"\n",
    "\n",
    "results = []\n",
    "# count = 48\n",
    "# max = len(data)\n",
    "for i, row in data.iterrows():\n",
    "    prompt, translated_text, score = construct_prompt(row, prompt_template)\n",
    "\n",
    "    #print(f\"Querying Gemini for row {i}...\")\n",
    "    response = query_gemini(prompt)\n",
    "\n",
    "    score_match = re.search(r\"Score:\\s*(\\d+)\", response)\n",
    "    if score_match:\n",
    "        score_match = int(score_match.group(1))\n",
    "    else:\n",
    "        score_match = None\n",
    "    correct_label = \"excellent\" if score_match == 5 else \"good\" if score_match >= 3 else \"poor\"\n",
    "\n",
    "    accuracy = re.search(r\"Accuracy:\\s*(.*)\", response)\n",
    "    fluency = re.search(r\"Fluency:\\s*(.*)\", response)\n",
    "    coherence = re.search(r\"Coherence:\\s*(.*)\", response)\n",
    "    cultural_appropriateness = re.search(r\"Cultural Appropriateness:\\s*(.*)\", response)\n",
    "    guideline_adherence = re.search(r\"Guideline Adherence:\\s*(.*)\", response)\n",
    "    completeness = re.search(r\"Completeness:\\s*(.*)\", response)\n",
    "\n",
    "    accuracy = accuracy.group(1).strip() if accuracy else \"\"\n",
    "    fluency = fluency.group(1).strip() if fluency else \"\"\n",
    "    coherence = coherence.group(1).strip() if coherence else \"\"\n",
    "    cultural_appropriateness = cultural_appropriateness.group(1).strip() if cultural_appropriateness else \"\"\n",
    "    guideline_adherence = guideline_adherence.group(1).strip() if guideline_adherence else \"\"\n",
    "    completeness = completeness.group(1).strip() if completeness else \"\"\n",
    "\n",
    "    comment1 = data.iloc[i]['Rater 1 Explanation']\n",
    "    comment2 = data.iloc[i]['Rater 2 Explanation']\n",
    "    \n",
    "\n",
    "    results.append({\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": response,\n",
    "        \"original_score\": int(score) if score is not None else None,\n",
    "        \"llm_score\": score_match,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"fluency\": fluency,\n",
    "        \"coherence\": coherence,\n",
    "        \"cultural_appropriateness\": cultural_appropriateness,\n",
    "        \"guideline_adherence\": guideline_adherence,\n",
    "        \"completeness\": completeness,\n",
    "        \"correct_label\": correct_label,\n",
    "        \"rater_1_comment\": comment1 if pd.notna(comment1) else \"\",\n",
    "        \"rater_2_comment\": comment2 if pd.notna(comment2) else \"\",\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "    time.sleep(5) \n",
    "\n",
    "import json\n",
    "with open(\"part1_results_7.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# 2.0 = results (sys 0), results_1 (sys 1), results_3 (system_3), results_5 (sys 4), results_6 (sys 5), results_7 (sys 6)\n",
    "# 2.5 = results_2 (sys1), results_4 (sys 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4423dfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "edited_results = []\n",
    "for i, row in enumerate(results):\n",
    "    prompt, response, score = row['prompt'], row['response'], row['original_score']\n",
    "\n",
    "    score_match = re.search(r\"Score:\\s*(\\d+)\", response)\n",
    "    if score_match:\n",
    "        score_match = int(score_match.group(1))\n",
    "    else:\n",
    "        score_match = None\n",
    "    correct_label = \"excellent\" if score_match == 5 else \"good\" if score_match >= 3 else \"poor\"\n",
    "\n",
    "    accuracy = re.search(r\"Accuracy:\\s*(.*)\", response)\n",
    "    fluency = re.search(r\"Fluency:\\s*(.*)\", response)\n",
    "    coherence = re.search(r\"Coherence:\\s*(.*)\", response)\n",
    "    cultural_appropriateness = re.search(r\"Cultural Appropriateness:\\s*(.*)\", response)\n",
    "    guideline_adherence = re.search(r\"Guideline Adherence:\\s*(.*)\", response)\n",
    "    completeness = re.search(r\"Completeness:\\s*(.*)\", response)\n",
    "\n",
    "    accuracy = accuracy.group(1).strip() if accuracy else \"\"\n",
    "    fluency = fluency.group(1).strip() if fluency else \"\"\n",
    "    coherence = coherence.group(1).strip() if coherence else \"\"\n",
    "    cultural_appropriateness = cultural_appropriateness.group(1).strip() if cultural_appropriateness else \"\"\n",
    "    guideline_adherence = guideline_adherence.group(1).strip() if guideline_adherence else \"\"\n",
    "    completeness = completeness.group(1).strip() if completeness else \"\"\n",
    "\n",
    "    comment1 = data.iloc[i]['Rater 1 Explanation']\n",
    "    comment2 = data.iloc[i]['Rater 2 Explanation']\n",
    "    \n",
    "\n",
    "    edited_results.append({\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": response,\n",
    "        \"original_score\": int(score) if score is not None else None,\n",
    "        \"llm_score\": score_match,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"fluency\": fluency,\n",
    "        \"coherence\": coherence,\n",
    "        \"cultural_appropriateness\": cultural_appropriateness,\n",
    "        \"guideline_adherence\": guideline_adherence,\n",
    "        \"completeness\": completeness,\n",
    "        \"correct_label\": correct_label,\n",
    "        \"rater_1_comment\": comment1 if pd.notna(comment1) else \"\",\n",
    "        \"rater_2_comment\": comment2 if pd.notna(comment2) else \"\",\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1767e867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation between human and LLM scores: 0.4591, p-value: 0.0003\n",
      "Exact agreement between human and LLM scores: 31.58%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "human_scores = [row['original_score'] for row in results if row['original_score'] is not None]\n",
    "llm_scores = [row['llm_score'] for row in results if row['llm_score'] is not None]\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "rho, pval = spearmanr(human_scores, llm_scores)\n",
    "print(f\"Spearman correlation between human and LLM scores: {rho:.4f}, p-value: {pval:.4f}\")\n",
    "\n",
    "agreement = sum(1 for h, l in zip(human_scores, llm_scores) if h == l) / len(human_scores)\n",
    "print(f\"Exact agreement between human and LLM scores: {agreement:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d819149f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spearman correlation for part1_results: 0.4616, p-value: 0.0003\n",
      "Pearson correlation for part1_results: 0.4267, p-value: 0.0009\n",
      "RMSE for part1_results: 1.4388\n",
      "Exact agreement for part1_results: 31.58%\n",
      "Partial agreement for part1_results: 68.42%\n",
      "Agreement for score 1: 25.00% (1/4)\n",
      "Agreement for score 2: 0.00% (0/10)\n",
      "Agreement for score 3: 0.00% (0/15)\n",
      "Agreement for score 4: 42.86% (6/14)\n",
      "Agreement for score 5: 78.57% (11/14)\n",
      "\n",
      "Spearman correlation for part1_results_1: 0.4590, p-value: 0.0003\n",
      "Pearson correlation for part1_results_1: 0.4563, p-value: 0.0004\n",
      "RMSE for part1_results_1: 1.4985\n",
      "Exact agreement for part1_results_1: 29.82%\n",
      "Partial agreement for part1_results_1: 63.16%\n",
      "Agreement for score 1: 0.00% (0/4)\n",
      "Agreement for score 2: 20.00% (2/10)\n",
      "Agreement for score 3: 6.67% (1/15)\n",
      "Agreement for score 4: 14.29% (2/14)\n",
      "Agreement for score 5: 85.71% (12/14)\n",
      "\n",
      "Spearman correlation for part1_results_2: 0.3392, p-value: 0.0098\n",
      "Pearson correlation for part1_results_2: 0.3665, p-value: 0.0051\n",
      "RMSE for part1_results_2: 1.5672\n",
      "Exact agreement for part1_results_2: 31.58%\n",
      "Partial agreement for part1_results_2: 66.67%\n",
      "Agreement for score 1: 100.00% (4/4)\n",
      "Agreement for score 2: 0.00% (0/10)\n",
      "Agreement for score 3: 20.00% (3/15)\n",
      "Agreement for score 4: 7.14% (1/14)\n",
      "Agreement for score 5: 71.43% (10/14)\n",
      "\n",
      "Spearman correlation for part1_results_3: 0.4890, p-value: 0.0001\n",
      "Pearson correlation for part1_results_3: 0.4557, p-value: 0.0004\n",
      "RMSE for part1_results_3: 1.5218\n",
      "Exact agreement for part1_results_3: 28.07%\n",
      "Partial agreement for part1_results_3: 61.40%\n",
      "Agreement for score 1: 25.00% (1/4)\n",
      "Agreement for score 2: 10.00% (1/10)\n",
      "Agreement for score 3: 0.00% (0/15)\n",
      "Agreement for score 4: 14.29% (2/14)\n",
      "Agreement for score 5: 85.71% (12/14)\n",
      "\n",
      "Spearman correlation for part1_results_4: 0.3655, p-value: 0.0052\n",
      "Pearson correlation for part1_results_4: 0.3953, p-value: 0.0023\n",
      "RMSE for part1_results_4: 1.5560\n",
      "Exact agreement for part1_results_4: 29.82%\n",
      "Partial agreement for part1_results_4: 68.42%\n",
      "Agreement for score 1: 100.00% (4/4)\n",
      "Agreement for score 2: 0.00% (0/10)\n",
      "Agreement for score 3: 20.00% (3/15)\n",
      "Agreement for score 4: 0.00% (0/14)\n",
      "Agreement for score 5: 71.43% (10/14)\n",
      "\n",
      "Spearman correlation for part1_results_5: 0.4850, p-value: 0.0001\n",
      "Pearson correlation for part1_results_5: 0.4386, p-value: 0.0006\n",
      "RMSE for part1_results_5: 1.5218\n",
      "Exact agreement for part1_results_5: 28.07%\n",
      "Partial agreement for part1_results_5: 61.40%\n",
      "Agreement for score 1: 25.00% (1/4)\n",
      "Agreement for score 2: 10.00% (1/10)\n",
      "Agreement for score 3: 0.00% (0/15)\n",
      "Agreement for score 4: 14.29% (2/14)\n",
      "Agreement for score 5: 85.71% (12/14)\n",
      "\n",
      "Spearman correlation for part1_results_6: 0.4922, p-value: 0.0001\n",
      "Pearson correlation for part1_results_6: 0.4962, p-value: 0.0001\n",
      "RMSE for part1_results_6: 1.4510\n",
      "Exact agreement for part1_results_6: 29.82%\n",
      "Partial agreement for part1_results_6: 64.91%\n",
      "Agreement for score 1: 0.00% (0/4)\n",
      "Agreement for score 2: 10.00% (1/10)\n",
      "Agreement for score 3: 6.67% (1/15)\n",
      "Agreement for score 4: 21.43% (3/14)\n",
      "Agreement for score 5: 85.71% (12/14)\n",
      "\n",
      "Spearman correlation for part1_results_7: 0.4591, p-value: 0.0003\n",
      "Pearson correlation for part1_results_7: 0.4874, p-value: 0.0001\n",
      "RMSE for part1_results_7: 1.5839\n",
      "Exact agreement for part1_results_7: 31.58%\n",
      "Partial agreement for part1_results_7: 57.89%\n",
      "Agreement for score 1: 50.00% (2/4)\n",
      "Agreement for score 2: 10.00% (1/10)\n",
      "Agreement for score 3: 0.00% (0/15)\n",
      "Agreement for score 4: 14.29% (2/14)\n",
      "Agreement for score 5: 92.86% (13/14)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from math import sqrt\n",
    "from statistics import mean\n",
    "\n",
    "filenames = [\n",
    "    \"part1_results\", \"part1_results_1\", \"part1_results_2\", \n",
    "    \"part1_results_3\", \"part1_results_4\", \"part1_results_5\", \n",
    "    \"part1_results_6\", \"part1_results_7\", \n",
    "]\n",
    "\n",
    "for filename in filenames:\n",
    "    with open(f\"{filename}.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        results = json.load(f)\n",
    "\n",
    "    llm_scores = [row['llm_score'] for row in results if row['llm_score'] is not None]\n",
    "    human_scores = [row['original_score'] for row in results if row['original_score'] is not None]\n",
    "\n",
    "    rho, pval_s = spearmanr(human_scores, llm_scores)\n",
    "    print(f\"\\nSpearman correlation for {filename}: {rho:.4f}, p-value: {pval_s:.4f}\")\n",
    "\n",
    "    r, pval_p = pearsonr(human_scores, llm_scores)\n",
    "    print(f\"Pearson correlation for {filename}: {r:.4f}, p-value: {pval_p:.4f}\")\n",
    "\n",
    "    rmse = sqrt(mean((h - l) ** 2 for h, l in zip(human_scores, llm_scores)))\n",
    "    print(f\"RMSE for {filename}: {rmse:.4f}\")\n",
    "\n",
    "    agreement = sum(1 for h, l in zip(human_scores, llm_scores) if h == l) / len(human_scores)\n",
    "    print(f\"Exact agreement for {filename}: {agreement:.2%}\")\n",
    "\n",
    "    partial_agreement = sum(1 for h, l in zip(human_scores, llm_scores) if abs(h - l) <= 1) / len(human_scores)\n",
    "    print(f\"Partial agreement for {filename}: {partial_agreement:.2%}\")\n",
    "\n",
    "    for score_value in range(1, 6):\n",
    "        count = sum(1 for h, l in zip(human_scores, llm_scores) if h == score_value and l == score_value)\n",
    "        total = sum(1 for h in human_scores if h == score_value)\n",
    "        agreement = count / total if total > 0 else 0\n",
    "        print(f\"Agreement for score {score_value}: {agreement:.2%} ({count}/{total})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ca918a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All rows have all criteria.\n"
     ]
    }
   ],
   "source": [
    "flag = False\n",
    "for row in results:\n",
    "    if any(not row.get(k, \"\").strip() for k in [\n",
    "        \"accuracy\", \"fluency\", \"coherence\",\n",
    "        \"cultural_appropriateness\", \"guideline_adherence\", \"completeness\"\n",
    "    ]):\n",
    "        print(f\"Row {row['row_index']} is missing some criteria.\")\n",
    "        flag = True\n",
    "\n",
    "if not flag:\n",
    "    print(\"All rows have all criteria.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9acdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consistency run 1 for row 40...\n",
      "Consistency run 2 for row 40...\n",
      "Consistency run 3 for row 40...\n",
      "Consistency run 4 for row 40...\n",
      "Consistency run 5 for row 40...\n",
      "Consistency run 1 for row 7...\n",
      "Consistency run 2 for row 7...\n",
      "Consistency run 3 for row 7...\n",
      "Consistency run 4 for row 7...\n",
      "Consistency run 5 for row 7...\n",
      "Consistency run 1 for row 1...\n",
      "Consistency run 2 for row 1...\n",
      "Consistency run 3 for row 1...\n",
      "Consistency run 4 for row 1...\n",
      "Consistency run 5 for row 1...\n",
      "Consistency run 1 for row 47...\n",
      "Consistency run 2 for row 47...\n",
      "Consistency run 3 for row 47...\n",
      "Consistency run 4 for row 47...\n",
      "Consistency run 5 for row 47...\n",
      "Consistency run 1 for row 17...\n",
      "Consistency run 2 for row 17...\n",
      "Consistency run 3 for row 17...\n",
      "Consistency run 4 for row 17...\n",
      "Consistency run 5 for row 17...\n",
      "Consistency run 1 for row 15...\n",
      "Consistency run 2 for row 15...\n",
      "Consistency run 3 for row 15...\n",
      "Consistency run 4 for row 15...\n",
      "Consistency run 5 for row 15...\n",
      "Consistency run 1 for row 14...\n",
      "Consistency run 2 for row 14...\n",
      "Consistency run 3 for row 14...\n",
      "Consistency run 4 for row 14...\n",
      "Consistency run 5 for row 14...\n",
      "Consistency run 1 for row 8...\n",
      "Consistency run 2 for row 8...\n",
      "Consistency run 3 for row 8...\n",
      "Consistency run 4 for row 8...\n",
      "Consistency run 5 for row 8...\n",
      "Consistency run 1 for row 53...\n",
      "Consistency run 2 for row 53...\n",
      "Consistency run 3 for row 53...\n",
      "Consistency run 4 for row 53...\n",
      "Consistency run 5 for row 53...\n",
      "Consistency run 1 for row 6...\n",
      "Consistency run 2 for row 6...\n",
      "Consistency run 3 for row 6...\n",
      "Consistency run 4 for row 6...\n",
      "Consistency run 5 for row 6...\n",
      "Consistency run 1 for row 43...\n",
      "Consistency run 2 for row 43...\n",
      "Consistency run 3 for row 43...\n",
      "Consistency run 4 for row 43...\n",
      "Consistency run 5 for row 43...\n",
      "Consistency run 1 for row 34...\n",
      "Consistency run 2 for row 34...\n",
      "Consistency run 3 for row 34...\n",
      "Consistency run 4 for row 34...\n",
      "Consistency run 5 for row 34...\n",
      "Consistency run 1 for row 5...\n",
      "Consistency run 2 for row 5...\n",
      "Consistency run 3 for row 5...\n",
      "Consistency run 4 for row 5...\n",
      "Consistency run 5 for row 5...\n",
      "Consistency run 1 for row 37...\n",
      "Consistency run 2 for row 37...\n",
      "Consistency run 3 for row 37...\n",
      "Consistency run 4 for row 37...\n",
      "Consistency run 5 for row 37...\n",
      "Consistency run 1 for row 27...\n",
      "Consistency run 2 for row 27...\n",
      "Consistency run 3 for row 27...\n",
      "Consistency run 4 for row 27...\n",
      "Consistency run 5 for row 27...\n",
      "Average deviation across sample: 5.72%\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "import os\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"\"\n",
    "\n",
    "def query_gemini_consistency(prompt):\n",
    "    client = genai.Client(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
    "    model = \"gemini-2.5-flash\"\n",
    "\n",
    "    contents = [\n",
    "        types.Content(\n",
    "            role=\"user\",\n",
    "            parts=[types.Part.from_text(text=prompt)],\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    generate_content_config = types.GenerateContentConfig(\n",
    "        temperature=0.35, \n",
    "        system_instruction=system,\n",
    "        response_mime_type=\"text/plain\",\n",
    "    )\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=model,\n",
    "        contents=contents,\n",
    "        config=generate_content_config,\n",
    "    )\n",
    "\n",
    "    return response.text.strip() if response and response.text else \"unknown\"\n",
    "\n",
    "\n",
    "sample_size = 15\n",
    "random.seed(42)\n",
    "sample_indices = random.sample(range(len(data)), sample_size)\n",
    "\n",
    "consistency_results = {}\n",
    "\n",
    "for idx in sample_indices:\n",
    "    row = data.iloc[idx]\n",
    "    prompt, translated_text, score = construct_prompt(row, prompt_template)\n",
    "    scores = []\n",
    "\n",
    "    for run in range(5):\n",
    "        print(f\"Consistency run {run+1} for row {idx}...\")\n",
    "        response = query_gemini_consistency(prompt)\n",
    "\n",
    "        score_match = re.search(r\"Score:\\s*(\\d+)\", response)\n",
    "        if score_match:\n",
    "            scores.append(int(score_match.group(1)))\n",
    "        else:\n",
    "            scores.append(None)\n",
    "\n",
    "        time.sleep(5) \n",
    "\n",
    "    consistency_results[idx] = {\n",
    "        \"prompt\": prompt,\n",
    "        \"scores\": scores\n",
    "    }\n",
    "\n",
    "deviations = []\n",
    "for idx, result in consistency_results.items():\n",
    "    valid_scores = [s for s in result[\"scores\"] if s is not None]\n",
    "    if len(valid_scores) > 1:\n",
    "        mean_score = np.mean(valid_scores)\n",
    "        std_dev = np.std(valid_scores)\n",
    "        deviation_percent = (std_dev / mean_score) * 100 if mean_score != 0 else 0\n",
    "        deviations.append(deviation_percent)\n",
    "\n",
    "overall_deviation = np.mean(deviations) if deviations else None\n",
    "print(f\"Average deviation across sample: {overall_deviation:.2f}%\")\n",
    "\n",
    "with open(\"consistency_results_1.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(consistency_results, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ab2217de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average deviation across sample: 1.27%\n"
     ]
    }
   ],
   "source": [
    "overall_deviation = np.mean(deviations) if deviations else None\n",
    "print(f\"Average deviation across sample: {overall_deviation:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474db050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index    Mean Score   Std Dev    Deviation %  Scores\n",
      "------------------------------------------------------------\n",
      "40       5.00         0.00       0.00         [5, 5, 5, 5, 5]\n",
      "7        5.00         0.00       0.00         [5, 5, 5, 5, 5]\n",
      "1        4.60         0.49       10.65        [5, 4, 5, 5, 4]\n",
      "47       4.00         0.00       0.00         [4, 4, 4, 4, 4]\n",
      "17       5.00         0.00       0.00         [5, 5, 5, 5, 5]\n",
      "15       4.00         0.00       0.00         [4, 4, 4, 4, 4]\n",
      "14       3.00         0.00       0.00         [3, 3, 3, 3, 3]\n",
      "8        5.00         0.00       0.00         [5, 5, 5, 5, 5]\n",
      "53       4.80         0.40       8.33         [5, 5, 5, 4, 5]\n",
      "6        5.00         0.00       0.00         [5, 5, 5, 5, 5]\n",
      "43       4.00         0.00       0.00         [4, 4, 4, 4, 4]\n",
      "34       5.00         0.00       0.00         [5, 5, 5, 5, 5]\n",
      "5        1.00         0.00       0.00         [1, 1, 1, 1, 1]\n",
      "37       5.00         0.00       0.00         [5, 5, 5, 5, 5]\n",
      "27       4.00         0.00       0.00         [4, 4, 4, 4, 4]\n",
      "------------------------------------------------------------\n",
      "Overall average deviation: 1.27%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "with open(\"consistency_results.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    consistency_results = json.load(f)\n",
    "\n",
    "print(f\"{'Index':<8} {'Mean Score':<12} {'Std Dev':<10} {'Deviation %':<12} {'Scores'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "deviations = []\n",
    "\n",
    "for idx, result in consistency_results.items():\n",
    "    scores = [s for s in result[\"scores\"] if s is not None]\n",
    "    if len(scores) > 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        std_dev = np.std(scores)\n",
    "        deviation_percent = (std_dev / mean_score) * 100 if mean_score != 0 else 0\n",
    "        deviations.append(deviation_percent)\n",
    "        print(f\"{idx:<8} {mean_score:<12.2f} {std_dev:<10.2f} {deviation_percent:<12.2f} {scores}\")\n",
    "\n",
    "if deviations:\n",
    "    overall_deviation = np.mean(deviations)\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Overall average deviation: {overall_deviation:.2f}%\")\n",
    "else:\n",
    "    print(\"No valid scores found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "570a1d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index    Mean Score   Std Dev    Deviation %  Scores\n",
      "------------------------------------------------------------\n",
      "40       5.00         0.00       0.00         [5, 5, 5, 5, 5]\n",
      "7        5.00         0.00       0.00         [5, 5, 5, 5, 5]\n",
      "1        5.00         0.00       0.00         [5, 5, 5, 5, 5]\n",
      "47       5.00         0.00       0.00         [5, 5, 5, 5, 5]\n",
      "17       5.00         0.00       0.00         [5, 5, 5, 5, 5]\n",
      "15       2.60         0.80       30.77        [3, 3, 3, 1, 3]\n",
      "14       3.00         0.00       0.00         [3, 3, 3, 3, 3]\n",
      "8        5.00         0.00       0.00         [5, 5, 5, 5, 5]\n",
      "53       3.60         0.80       22.22        [4, 5, 3, 3, 3]\n",
      "6        3.60         0.80       22.22        [3, 3, 5, 4, 3]\n",
      "43       3.00         0.00       0.00         [3, 3, 3, 3, 3]\n",
      "34       3.80         0.40       10.53        [4, 4, 3, 4, 4]\n",
      "5        1.00         0.00       0.00         [1, 1, 1, 1, 1]\n",
      "37       5.00         0.00       0.00         [5, 5, 5, 5, 5]\n",
      "27       5.00         0.00       0.00         [5, 5, 5, 5, 5]\n",
      "------------------------------------------------------------\n",
      "Overall average deviation: 5.72%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "with open(\"consistency_results_1.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    consistency_results = json.load(f)\n",
    "\n",
    "print(f\"{'Index':<8} {'Mean Score':<12} {'Std Dev':<10} {'Deviation %':<12} {'Scores'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "deviations = []\n",
    "\n",
    "for idx, result in consistency_results.items():\n",
    "    scores = [s for s in result[\"scores\"] if s is not None]\n",
    "    if len(scores) > 1:\n",
    "        mean_score = np.mean(scores)\n",
    "        std_dev = np.std(scores)\n",
    "        deviation_percent = (std_dev / mean_score) * 100 if mean_score != 0 else 0\n",
    "        deviations.append(deviation_percent)\n",
    "        print(f\"{idx:<8} {mean_score:<12.2f} {std_dev:<10.2f} {deviation_percent:<12.2f} {scores}\")\n",
    "\n",
    "if deviations:\n",
    "    overall_deviation = np.mean(deviations)\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Overall average deviation: {overall_deviation:.2f}%\")\n",
    "else:\n",
    "    print(\"No valid scores found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b55c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-10 23:57:48 [__init__.py:235] Automatically detected platform cuda.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vllm._C'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLM\n\u001b[0;32m      3\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./ByteDance-Seed/Seed-X-PPO-7B\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m LLM(model\u001b[38;5;241m=\u001b[39mmodel_path,\n\u001b[0;32m      6\u001b[0m             max_num_seqs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[0;32m      7\u001b[0m             tensor_parallel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m             )\n",
      "File \u001b[1;32md:\\conda_envs\\myenv\\Lib\\site-packages\\vllm\\__init__.py:64\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m MODULE_ATTRS:\n\u001b[0;32m     63\u001b[0m     module_name, attr_name \u001b[38;5;241m=\u001b[39m MODULE_ATTRS[name]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 64\u001b[0m     module \u001b[38;5;241m=\u001b[39m import_module(module_name, __package__)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, attr_name)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\conda_envs\\myenv\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32md:\\conda_envs\\myenv\\Lib\\site-packages\\vllm\\entrypoints\\llm.py:20\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TypeVar, deprecated\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbeam_search\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (BeamSearchInstance, BeamSearchOutput,\n\u001b[0;32m     18\u001b[0m                               BeamSearchSequence,\n\u001b[0;32m     19\u001b[0m                               create_sort_beams_key_function)\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (CompilationConfig, ModelDType, TokenizerMode,\n\u001b[0;32m     21\u001b[0m                          is_init_field)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marg_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (EngineArgs, HfOverrides, PoolerConfig,\n\u001b[0;32m     23\u001b[0m                                    TaskOption)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm_engine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLMEngine\n",
      "File \u001b[1;32md:\\conda_envs\\myenv\\Lib\\site-packages\\vllm\\config.py:35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompilation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minductor_pass\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CallableInductorPass, InductorPass\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_logger\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_executor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QuantizationMethods\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m current_platform\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformers_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     38\u001b[0m     ConfigFormat, get_config, get_hf_image_processor_config,\n\u001b[0;32m     39\u001b[0m     get_hf_text_config, get_pooling_config,\n\u001b[0;32m     40\u001b[0m     get_sentence_transformer_tokenizer_config, is_encoder_decoder,\n\u001b[0;32m     41\u001b[0m     try_get_generation_config, try_get_safetensors_metadata,\n\u001b[0;32m     42\u001b[0m     try_get_tokenizer_config, uses_mrope)\n",
      "File \u001b[1;32md:\\conda_envs\\myenv\\Lib\\site-packages\\vllm\\model_executor\\__init__.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# SPDX-License-Identifier: Apache-2.0\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_executor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparameter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (BasevLLMParameter,\n\u001b[0;32m      5\u001b[0m                                            PackedvLLMParameter)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_executor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msampling_metadata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (SamplingMetadata,\n\u001b[0;32m      7\u001b[0m                                                    SamplingMetadataCache)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_executor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_random_seed\n",
      "File \u001b[1;32md:\\conda_envs\\myenv\\Lib\\site-packages\\vllm\\model_executor\\parameter.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Parameter\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tensor_model_parallel_rank\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_logger\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_executor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _make_synced_weight_loader\n",
      "File \u001b[1;32md:\\conda_envs\\myenv\\Lib\\site-packages\\vllm\\distributed\\__init__.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# SPDX-License-Identifier: Apache-2.0\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommunication_op\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel_state\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32md:\\conda_envs\\myenv\\Lib\\site-packages\\vllm\\distributed\\communication_op.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel_state\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tp_group\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtensor_model_parallel_all_reduce\u001b[39m(input_: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m     13\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"All-reduce the input tensor across model parallel group.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32md:\\conda_envs\\myenv\\Lib\\site-packages\\vllm\\distributed\\parallel_state.py:150\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mempty(new_shape, dtype\u001b[38;5;241m=\u001b[39mtensor\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mtensor\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m supports_custom_op():\n\u001b[1;32m--> 150\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m current_platform\n\u001b[0;32m    151\u001b[0m     direct_register_custom_op(\n\u001b[0;32m    152\u001b[0m         op_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall_reduce\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    153\u001b[0m         op_func\u001b[38;5;241m=\u001b[39mall_reduce,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    156\u001b[0m         dispatch_key\u001b[38;5;241m=\u001b[39mcurrent_platform\u001b[38;5;241m.\u001b[39mdispatch_key,\n\u001b[0;32m    157\u001b[0m     )\n\u001b[0;32m    159\u001b[0m     direct_register_custom_op(\n\u001b[0;32m    160\u001b[0m         op_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreduce_scatter\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    161\u001b[0m         op_func\u001b[38;5;241m=\u001b[39mreduce_scatter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    164\u001b[0m         dispatch_key\u001b[38;5;241m=\u001b[39mcurrent_platform\u001b[38;5;241m.\u001b[39mdispatch_key,\n\u001b[0;32m    165\u001b[0m     )\n",
      "File \u001b[1;32md:\\conda_envs\\myenv\\Lib\\site-packages\\vllm\\platforms\\__init__.py:267\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _current_platform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    266\u001b[0m     platform_cls_qualname \u001b[38;5;241m=\u001b[39m resolve_current_platform_cls_qualname()\n\u001b[1;32m--> 267\u001b[0m     _current_platform \u001b[38;5;241m=\u001b[39m resolve_obj_by_qualname(\n\u001b[0;32m    268\u001b[0m         platform_cls_qualname)()\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28;01mglobal\u001b[39;00m _init_trace\n\u001b[0;32m    270\u001b[0m     _init_trace \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(traceback\u001b[38;5;241m.\u001b[39mformat_stack())\n",
      "File \u001b[1;32md:\\conda_envs\\myenv\\Lib\\site-packages\\vllm\\utils\\__init__.py:2539\u001b[0m, in \u001b[0;36mresolve_obj_by_qualname\u001b[1;34m(qualname)\u001b[0m\n\u001b[0;32m   2535\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2536\u001b[0m \u001b[38;5;124;03mResolve an object by its fully qualified name.\u001b[39;00m\n\u001b[0;32m   2537\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2538\u001b[0m module_name, obj_name \u001b[38;5;241m=\u001b[39m qualname\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m-> 2539\u001b[0m module \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(module_name)\n\u001b[0;32m   2540\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, obj_name)\n",
      "File \u001b[1;32md:\\conda_envs\\myenv\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32md:\\conda_envs\\myenv\\Lib\\site-packages\\vllm\\platforms\\cuda.py:18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParamSpec\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# import custom ops, trigger op registration\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01menvs\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_logger\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'vllm._C'"
     ]
    }
   ],
   "source": [
    "# stupid thing that doesnt work on windows waste of time\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "model_path = \"./ByteDance-Seed/Seed-X-PPO-7B\"\n",
    "\n",
    "model = LLM(model=model_path,\n",
    "            max_num_seqs=512,\n",
    "            tensor_parallel_size=8,\n",
    "            enable_prefix_caching=True, \n",
    "            gpu_memory_utilization=0.95,\n",
    "\n",
    "            )\n",
    "\n",
    "\n",
    "def back_translate(filipino_text: str) -> str:\n",
    "    messages = [\n",
    "        \"Translate the following Filipino sentence into English:\\nAng ganda mo!<en>\",\n",
    "    ]\n",
    "\n",
    "    # Beam search (We recommend using beam search decoding)\n",
    "    decoding_params = BeamSearchParams(beam_width=4,\n",
    "                                    max_tokens=512)\n",
    "    # Greedy decoding\n",
    "    decoding_params = SamplingParams(temperature=0,\n",
    "                                    max_tokens=512,\n",
    "                                    skip_special_tokens=True)\n",
    "\n",
    "    results = model.generate(messages, decoding_params)\n",
    "    responses = [res.outputs[0].text.strip() for res in results]\n",
    "\n",
    "    print(responses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9ea0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda_envs\\myenv\\Lib\\site-packages\\transformers\\utils\\hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "d:\\conda_envs\\myenv\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:935: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "d:\\conda_envs\\myenv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4f6fc9611564f8e827a061dbdc4c3dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ.setdefault(\"GEMINI_API_KEY\", \"\")\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"facebook/nllb-200-3.3B\",\n",
    "    src_lang=\"tgl_Latn\",  \n",
    "    use_auth_token=True,\n",
    "    cache_dir=\"D:/_GitRepos/Thesis/huggingface_cache\"\n",
    ")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"facebook/nllb-200-3.3B\",\n",
    "    use_auth_token=True,\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    cache_dir=\"D:/_GitRepos/Thesis/huggingface_cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f620ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for row: 0\n",
      "Running evaluation for row: 1\n",
      "Running evaluation for row: 2\n",
      "Running evaluation for row: 3\n",
      "Running evaluation for row: 4\n",
      "Running evaluation for row: 5\n",
      "Running evaluation for row: 6\n",
      "Running evaluation for row: 7\n",
      "Running evaluation for row: 8\n",
      "Running evaluation for row: 9\n",
      "Running evaluation for row: 10\n",
      "Running evaluation for row: 11\n",
      "Running evaluation for row: 12\n",
      "Running evaluation for row: 13\n",
      "Running evaluation for row: 14\n",
      "Running evaluation for row: 15\n",
      "Running evaluation for row: 16\n",
      "Running evaluation for row: 17\n",
      "Running evaluation for row: 18\n",
      "Running evaluation for row: 19\n",
      "Running evaluation for row: 20\n",
      "Running evaluation for row: 21\n",
      "Running evaluation for row: 22\n",
      "Running evaluation for row: 23\n",
      "Running evaluation for row: 24\n",
      "Running evaluation for row: 25\n",
      "Running evaluation for row: 26\n",
      "Running evaluation for row: 27\n",
      "Running evaluation for row: 28\n",
      "Running evaluation for row: 29\n",
      "Running evaluation for row: 30\n",
      "Running evaluation for row: 31\n",
      "Running evaluation for row: 32\n",
      "Running evaluation for row: 33\n",
      "Running evaluation for row: 34\n",
      "Running evaluation for row: 35\n",
      "Running evaluation for row: 36\n",
      "Running evaluation for row: 37\n",
      "Running evaluation for row: 38\n",
      "Running evaluation for row: 39\n",
      "Running evaluation for row: 40\n",
      "Running evaluation for row: 41\n",
      "Running evaluation for row: 42\n",
      "Running evaluation for row: 43\n",
      "Running evaluation for row: 44\n",
      "Running evaluation for row: 45\n",
      "Running evaluation for row: 46\n",
      "Running evaluation for row: 47\n",
      "Running evaluation for row: 48\n",
      "Running evaluation for row: 49\n",
      "Running evaluation for row: 50\n",
      "Running evaluation for row: 51\n",
      "Running evaluation for row: 52\n",
      "Running evaluation for row: 53\n",
      "Running evaluation for row: 54\n",
      "Running evaluation for row: 55\n",
      "Running evaluation for row: 56\n",
      "Saved 57 evaluation results to agentic_results_13_libretl.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from typing import List, Dict, Tuple\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import time\n",
    "os.environ.setdefault(\"GEMINI_API_KEY\", \"\")\n",
    "\n",
    "API_KEY = os.environ.get(\"GEMINI_API_KEY\")\n",
    "MODEL = \"gemini-2.5-flash-lite\"   \n",
    "RATE_LIMIT_SLEEP = 5 \n",
    "MAX_RETRIES = 3\n",
    "\n",
    "#from google.cloud import translate_v2 as translate\n",
    "# def back_translate(filipino_text: str) -> str:\n",
    "#     result = client.translate(filipino_text, target_language=\"en\", source_language=\"tl\")\n",
    "#     return result[\"translatedText\"]\n",
    "\n",
    "# client = translate.Client()\n",
    "\n",
    "import torch\n",
    "\n",
    "# def back_translate(filipino_text: str) -> str:\n",
    "#     inputs = tokenizer(filipino_text, return_tensors=\"pt\")\n",
    "#     if torch.cuda.is_available():\n",
    "#         inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "\n",
    "#     # Specify forced_bos_token_id for English output (\"eng_Latn\")\n",
    "#     forced_bos_token_id = tokenizer.convert_tokens_to_ids(\"eng_Latn\")\n",
    "\n",
    "#     outputs = model.generate(\n",
    "#         **inputs,\n",
    "#         forced_bos_token_id=forced_bos_token_id,\n",
    "#         max_length=128,\n",
    "#         do_sample=False,  # greedy decoding\n",
    "#     )\n",
    "\n",
    "#     translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#     return translated_text\n",
    "\n",
    "import requests\n",
    "# pip install libretranslate\n",
    "# libretranslate\n",
    "def back_translate(text: str) -> str:\n",
    "    source_lang = \"en\"\n",
    "    target_lang = \"tl\"\n",
    "    url = \"http://127.0.0.1:5000/translate\" #my localhost\n",
    "    data = {\n",
    "        \"q\": text,\n",
    "        \"source\": source_lang,\n",
    "        \"target\": target_lang,\n",
    "        \"format\": \"text\"\n",
    "    }\n",
    "    response = requests.post(url, data=data)\n",
    "    if response.status_code == 200:\n",
    "        translated = response.json().get(\"translatedText\", \"\")\n",
    "        return translated\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} {response.text}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "\n",
    "def call_gemini(system_instruction: str, user_prompt: str, temperature: float = 0.0) -> str:\n",
    "    client = genai.Client(api_key=API_KEY)\n",
    "    contents = [\n",
    "        types.Content(\n",
    "            role=\"user\",\n",
    "            parts=[types.Part.from_text(text=user_prompt)],\n",
    "        )\n",
    "    ]\n",
    "    config = types.GenerateContentConfig(\n",
    "        temperature=temperature,\n",
    "        system_instruction=system_instruction,\n",
    "        response_mime_type=\"text/plain\"\n",
    "    )\n",
    "\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            response = client.models.generate_content(\n",
    "                model=MODEL,\n",
    "                contents=contents,\n",
    "                config=config,\n",
    "            )\n",
    "            text = response.text or \"\"\n",
    "            return text.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"[call_gemini] Attempt {attempt+1} failed: {e}\")\n",
    "            time.sleep(RATE_LIMIT_SLEEP * (attempt + 1))\n",
    "    raise RuntimeError(\"call_gemini failed after retries\")\n",
    "\n",
    "\n",
    "def evaluate_pair(english: str, filipino: str, human_score: int, explanation1: str, explanation2: str) -> Dict:\n",
    "    import re\n",
    "\n",
    "    paraphrase_prompt = f\"\"\"\n",
    "    Paraphrase the following sentences concisely. The sentences are separate, do not think about them together. Only provide one paraphrase for each:\n",
    "\n",
    "    English sentence: {english}\n",
    "\n",
    "    Filipino sentence (translate and paraphrase into English): {filipino}\n",
    "\n",
    "    Respond with the paraphrases in this format:\n",
    "\n",
    "    English paraphrase: [your paraphrase here]\n",
    "    Filipino paraphrase (in English): [your paraphrase here]\n",
    "    \"\"\"\n",
    "\n",
    "    paraphrase_response = call_gemini(\n",
    "        system_instruction=\"You are a helpful paraphraser. You are intelligent. Simply do as instructed.\",\n",
    "        user_prompt=paraphrase_prompt\n",
    "    )\n",
    "\n",
    "    pattern = re.compile(\n",
    "        r\"English paraphrase:\\s*(?P<eng>.+?)\\s*Filipino paraphrase \\(in English\\):\\s*(?P<fil>.+)\", \n",
    "        re.DOTALL\n",
    "    )\n",
    "\n",
    "    match = pattern.search(paraphrase_response)\n",
    "    if match:\n",
    "        paraphrase_src = match.group(\"eng\").strip()\n",
    "        paraphrase_tr = match.group(\"fil\").strip()\n",
    "    else:\n",
    "        paraphrase_src = \"\"\n",
    "        paraphrase_tr = \"\"\n",
    "\n",
    "    back_translated = back_translate(filipino).strip()\n",
    "\n",
    "    time.sleep(5)  \n",
    "    scoring_prompt = f\"\"\"\n",
    "        Given the English source, Filipino translation, and various paraphrases, score the Filipino translation from 1 to 5 on these criteria:\n",
    "\n",
    "        Accuracy, Fluency, Coherence, Cultural Appropriateness, Completeness.\n",
    "\n",
    "        English source: {english}\n",
    "        Filipino translation: {filipino}\n",
    "        Back-translation: {back_translated}\n",
    "\n",
    "        Respond clearly with each criterion’s score and justification in this format:\n",
    "        Accuracy: 4.5 |  [your explanation here]\n",
    "        Fluency: 4.0 |  [your explanation here]\n",
    "        Coherence: 4.2 |  [your explanation here]\n",
    "        Cultural Appropriateness: 5.0 |  [your explanation here]\n",
    "        Completeness: 4.8 |  [your explanation here]\n",
    "        \"\"\"\n",
    "    scoring_response = call_gemini(\n",
    "        system_instruction=\"You are a helpful assistant. You are an expert translator and evaluator.\",\n",
    "        user_prompt=scoring_prompt\n",
    "    )\n",
    "\n",
    "    time.sleep(5)  \n",
    "    scores_and_justifications = {}\n",
    "    for line in scoring_response.splitlines():\n",
    "        if \": \" in line and \"|\" in line:\n",
    "            key_part, rest = line.split(\":\", 1)\n",
    "            score_part, justification_part = rest.split(\"|\", 1)\n",
    "            key = key_part.strip()\n",
    "            try:\n",
    "                score = float(score_part.strip())\n",
    "            except ValueError:\n",
    "                score = None\n",
    "            justification = justification_part.strip()\n",
    "            scores_and_justifications[key] = {\n",
    "                \"score\": score,\n",
    "                \"justification\": justification\n",
    "            }\n",
    "\n",
    "    summary_prompt = f\"\"\"\n",
    "        Based on these scores and justifications:\n",
    "\n",
    "        {scoring_response}\n",
    "\n",
    "        Provide an integer score from 1-5 of the Filipino translation, and a brief summary of the overall translation quality.\n",
    "        Score:\n",
    "        Summary:\n",
    "        \"\"\"\n",
    "\n",
    "    summary = call_gemini(\n",
    "        system_instruction=\"You are a helpful assistant.\",\n",
    "        user_prompt=summary_prompt\n",
    "    )\n",
    "\n",
    "    time.sleep(5) \n",
    "\n",
    "    return {\n",
    "        \"english\": english,\n",
    "        \"filipino\": filipino,\n",
    "        \"paraphrase_src\": paraphrase_src,\n",
    "        \"paraphrase_tr\": paraphrase_tr,\n",
    "        \"back_translated\": back_translated,\n",
    "        \"scores_and_justifications\": scores_and_justifications,\n",
    "        \"summary\": summary,\n",
    "        \"human_score\": human_score,\n",
    "        \"explanation1\": explanation1,\n",
    "        \"explanation2\": explanation2\n",
    "    }\n",
    "\n",
    "\n",
    "results = []\n",
    "traces = []\n",
    "def run_agentic_on_dataframe(df: pd.DataFrame,\n",
    "                             text_col_src: str,\n",
    "                             text_col_tr: str,\n",
    "                             out_json_path: str,\n",
    "                             max_rows: int = None) -> Tuple[List[Dict], List[Dict]]:\n",
    "    count = 0\n",
    "    min = 55\n",
    "\n",
    "    rows = df.head(max_rows) if max_rows else df\n",
    "    for idx, row in rows.iterrows():\n",
    "        # if count < min:\n",
    "        #     count += 1\n",
    "        #     continue\n",
    "        print(\"Running evaluation for row:\", idx)\n",
    "        english = str(row[text_col_src]).strip()\n",
    "        filipino = str(row[text_col_tr]).strip()\n",
    "        human_score = int(row.get(\"Score\", None))\n",
    "        explanation1 = str(row.get(\"Rater 1 Explanation\", \"\")).strip()\n",
    "        explanation2 = str(row.get(\"Rater 2 Explanation\", \"\")).strip()\n",
    "\n",
    "        eval_result = evaluate_pair(english, filipino, human_score, explanation1, explanation2)\n",
    "        results.append(eval_result)\n",
    "\n",
    "        traces.append({\n",
    "            \"row_index\": idx,\n",
    "            \"english\": english,\n",
    "            \"filipino\": filipino,\n",
    "            \"paraphrase_src\": eval_result[\"paraphrase_src\"],\n",
    "            \"paraphrase_tr\": eval_result[\"paraphrase_tr\"],\n",
    "            \"back_translated\": eval_result[\"back_translated\"]\n",
    "        })\n",
    "\n",
    "    with open(out_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    return results, traces\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = pd.read_csv(\"data1.csv\")\n",
    "\n",
    "    out_path = \"agentic_results_13_libretl.json\"\n",
    "    res, traces = run_agentic_on_dataframe(\n",
    "        data,\n",
    "        text_col_src=\"English\",\n",
    "        text_col_tr=\"Filipino\",\n",
    "        out_json_path=out_path,\n",
    "        max_rows= None\n",
    "    )\n",
    "    print(f\"Saved {len(res)} evaluation results to {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8819e54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spearman correlation for agentic_results_13_libretl.json: 0.4156, p-value: 0.0013\n",
      "Pearson correlation for agentic_results_13_libretl.json: 0.4005, p-value: 0.0020\n",
      "RMSE for agentic_results_13_libretl.json: 1.2283\n",
      "Exact agreement for agentic_results_13_libretl.json: 29.82%\n",
      "Partial agreement for agentic_results_13_libretl.json: 78.95%\n",
      "Agreement for score 1: 25.00% (1/4)\n",
      "Agreement for score 2: 0.00% (0/10)\n",
      "Agreement for score 3: 0.00% (0/15)\n",
      "Agreement for score 4: 100.00% (14/14)\n",
      "Agreement for score 5: 14.29% (2/14)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from math import sqrt\n",
    "from statistics import mean\n",
    "import re\n",
    "\n",
    "filename = \"agentic_results_13_libretl.json\"\n",
    "\n",
    "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "    agentic_results_3 = json.load(f)\n",
    "\n",
    "human_scores = []\n",
    "llm_scores = []\n",
    "\n",
    "for row in agentic_results_3:\n",
    "    if row.get(\"human_score\") is None:\n",
    "        continue \n",
    "    match = re.search(r\"Score:\\s*(\\d+)\", row.get(\"summary\", \"\"))\n",
    "    if match:\n",
    "        llm_score = int(match.group(1))\n",
    "        llm_scores.append(llm_score)\n",
    "        human_scores.append(row[\"human_score\"])\n",
    "\n",
    "assert len(human_scores) == len(llm_scores), \"Mismatch in score counts!\"\n",
    "\n",
    "rho, pval_s = spearmanr(human_scores, llm_scores)\n",
    "print(f\"\\nSpearman correlation for {filename}: {rho:.4f}, p-value: {pval_s:.4f}\")\n",
    "\n",
    "r, pval_p = pearsonr(human_scores, llm_scores)\n",
    "print(f\"Pearson correlation for {filename}: {r:.4f}, p-value: {pval_p:.4f}\")\n",
    "\n",
    "rmse = sqrt(mean((h - l) ** 2 for h, l in zip(human_scores, llm_scores)))\n",
    "print(f\"RMSE for {filename}: {rmse:.4f}\")\n",
    "\n",
    "agreement = sum(1 for h, l in zip(human_scores, llm_scores) if h == l) / len(human_scores)\n",
    "print(f\"Exact agreement for {filename}: {agreement:.2%}\")\n",
    "\n",
    "partial_agreement = sum(1 for h, l in zip(human_scores, llm_scores) if abs(h - l) <= 1) / len(human_scores)\n",
    "print(f\"Partial agreement for {filename}: {partial_agreement:.2%}\")\n",
    "\n",
    "for score_value in range(1, 6):\n",
    "    count = sum(1 for h, l in zip(human_scores, llm_scores) if h == score_value and l == score_value)\n",
    "    total = sum(1 for h in human_scores if h == score_value)\n",
    "    agreement = count / total if total > 0 else 0\n",
    "    print(f\"Agreement for score {score_value}: {agreement:.2%} ({count}/{total})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
